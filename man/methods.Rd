% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{methods}
\alias{methods}
\alias{glm.common}
\alias{predict.glm.common}
\alias{coef.glm.common}
\alias{glm.spls}
\alias{predict.glm.spls}
\alias{coef.glm.spls}
\alias{glm.xrnet}
\alias{predict.glm.xrnet}
\alias{coef.glm.xrnet}
\alias{glm.empty}
\alias{glm.separate}
\alias{predict.glm.separate}
\alias{coef.glm.separate}
\alias{glm.mgaussian}
\alias{predict.glm.mgaussian}
\alias{coef.glm.mgaussian}
\alias{glm.glmtrans}
\alias{predict.glm.glmtrans}
\alias{coef.glm.glmtrans}
\title{Available methods}
\usage{
glm.common(x, y, family, alpha = 1)

\method{predict}{glm.common}(object, newx)

\method{coef}{glm.common}(object)

glm.spls(x, y, family = "gaussian", alpha = 1, nfolds = 10)

\method{predict}{glm.spls}(object, newx)

\method{coef}{glm.spls}(object)

glm.xrnet(x, y, alpha.init = 0.95, alpha = 1, nfolds = 10, family = "gaussian")

\method{predict}{glm.xrnet}(object, newx)

\method{coef}{glm.xrnet}(object)

glm.empty(x, y, family, alpha = 1)

glm.separate(x, y, family, alpha = 1, lambda = NULL)

\method{predict}{glm.separate}(object, newx)

\method{coef}{glm.separate}(object)

glm.mgaussian(x, y, family = "gaussian", alpha = 1)

\method{predict}{glm.mgaussian}(object, newx)

\method{coef}{glm.mgaussian}(object)

glm.glmtrans(x, y, family = "gaussian", alpha = 1)

\method{predict}{glm.glmtrans}(object, newx)

\method{coef}{glm.glmtrans}(object)
}
\arguments{
\item{x}{feature matrix (multi-task learning)
or list of \eqn{q} feature matrices (transfer learning)}

\item{y}{response matrix (multi-task learning)
or list of \eqn{q} response vectors (transfer learning)}

\item{family}{character vector with 1 or \eqn{q} entries,
possible values are \code{"gaussian"} and sometimes \code{"binomial"} or other}

\item{alpha}{elastic net mixing parameter:
number between 0 and 1}

\item{object}{output from multi-task learning or transfer learning method}

\item{newx}{feature matrix (MTL) or list of feature matrices (TL)
of testing samples}

\item{nfolds}{number of cross-validation folds: positive integer}

\item{alpha.init}{elastic net mixing parameter for initial models:
number between 0 and 1}

\item{lambda}{sequence of regularisation parameters}
}
\description{
Wrapper functions of available methods for related problems.
}
\examples{
# multi-task learning
n_train <- 100
n_test <- 10
p <- 50
q <- 3
family <- "gaussian"
x <- matrix(data=rnorm(n=n_train*p),nrow=n_train,ncol=p)
newx <- matrix(data=rnorm(n=n_test*p),nrow=n_test,ncol=p)
y <- matrix(data=rnorm(n_train*q),nrow=n_train,ncol=q)
object <- glm.empty(x=x,y=y,family=family)
object <- glm.separate(x=x,y=y,family=family)
object <- glm.mgaussian(x=x,y=y,family=family)
object <- glm.spls(x=x,y=y,family=family)
coef(object)
predict(object,newx=newx)

# transfer learning
n_train <- c(100,50)
n_test <- c(10,10)
p <- 50
x <- lapply(X=n_train,function(n) matrix(data=stats::rnorm(n*p),nrow=n,ncol=p))
newx <- lapply(X=n_test,function(n) matrix(data=stats::rnorm(n*p),nrow=n,ncol=p))
y <- lapply(X=n_train,function(n) stats::rnorm(n))
family <- "gaussian"
object <- glm.empty(x=x,y=y,family=family)
object <- glm.separate(x=x,y=y,family=family)
object <- glm.common(x=x,y=y,family=family)
object <- glm.glmtrans(x=x,y=y,family=family)
object <- glm.xrnet(x=x,y=y,family=family)
coef(object)
predict(object,newx=newx)

}
\keyword{internal}
